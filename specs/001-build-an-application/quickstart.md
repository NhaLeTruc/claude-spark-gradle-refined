# Quickstart Guide: Data Pipeline Orchestration Application

**Feature**: Data Pipeline Orchestration Application
**Date**: 2025-10-13
**Branch**: 001-build-an-application

This guide walks you through setting up, configuring, and running your first data pipeline.

## Prerequisites

- **Java**: JDK 11 or later
- **Scala**: 2.12.x (managed by Gradle)
- **Gradle**: 8.5+ (use included wrapper: `./gradlew`)
- **Docker**: For local testing environment
- **Docker Compose**: v2.x for orchestrating services

## Quick Setup (5 Minutes)

### 1. Clone and Build

```bash
# Clone repository
git clone https://github.com/your-org/claude-spark-gradle-two.git
cd claude-spark-gradle-two

# Build project
./gradlew build

# Run tests (optional)
./gradlew test
```

### 2. Start Local Environment

```bash
# Start Docker services (PostgreSQL, MySQL, Kafka, Vault, MinIO)
docker-compose up -d

# Verify services are healthy
docker-compose ps

# Expected output: All services show "Up" or "healthy" status
```

### 3. Configure Credentials

```bash
# Copy example environment file
cp .env.example .env

# Edit .env with your local credentials (generated by docker-compose)
nano .env
```

**Example `.env` file**:
```bash
# Vault Configuration
VAULT_ADDR=http://localhost:8200
VAULT_TOKEN=dev-root-token

# PostgreSQL Credentials (populated into Vault)
POSTGRES_HOST=localhost
POSTGRES_PORT=5432
POSTGRES_DATABASE=testdb
POSTGRES_USERNAME=testuser
POSTGRES_PASSWORD=testpass

# MySQL Credentials
MYSQL_HOST=localhost
MYSQL_PORT=3306
MYSQL_DATABASE=testdb
MYSQL_USERNAME=testuser
MYSQL_PASSWORD=testpass

# Kafka Configuration
KAFKA_BOOTSTRAP_SERVERS=localhost:9092

# S3/MinIO Configuration
S3_ACCESS_KEY=minioadmin
S3_SECRET_KEY=minioadmin
S3_REGION=us-east-1
S3_ENDPOINT=http://localhost:9000
```

### 4. Initialize Vault with Credentials

```bash
# Run Vault initialization script
./docker/vault/init-vault.sh

# Verify secrets are stored
docker exec -it vault-dev vault kv get secret/postgres/dev
```

Expected output:
```
====== Data ======
Key         Value
---         -----
database    testdb
host        localhost
password    testpass
port        5432
username    testuser
```

### 5. Run Your First Pipeline

```bash
# Execute simple ETL pipeline (PostgreSQL â†’ S3)
./gradlew run --args="config/examples/simple-etl.json"
```

Expected output:
```
[INFO] Starting pipeline: simple-postgres-to-s3 (mode: batch)
[INFO] Executing step: extract.fromPostgres
[INFO] Step completed: 1000 records in 523ms
[INFO] Executing step: transform.filterRows
[INFO] Step completed: 750 records in 89ms
[INFO] Executing step: load.toS3
[INFO] Step completed: 750 records in 412ms
[INFO] Pipeline completed successfully: simple-postgres-to-s3
```

## Pipeline Configuration

### Simple ETL Pipeline Example

**File**: `config/examples/simple-etl.json`

```json
{
  "name": "simple-postgres-to-s3",
  "executionMode": "batch",
  "steps": [
    {
      "stepType": "extract",
      "method": "fromPostgres",
      "config": {
        "table": "users",
        "credentialPath": "secret/data/postgres/dev",
        "credentialType": "jdbc"
      }
    },
    {
      "stepType": "transform",
      "method": "filterRows",
      "config": {
        "condition": "age >= 18 AND active = true"
      }
    },
    {
      "stepType": "load",
      "method": "toS3",
      "config": {
        "bucket": "pipeline-output",
        "path": "users/filtered/",
        "format": "parquet",
        "mode": "Overwrite",
        "credentialPath": "secret/data/s3/dev",
        "credentialType": "iam"
      }
    }
  ]
}
```

### Multi-Source Complex Pipeline

**File**: `config/examples/multi-source-pipeline.json`

```json
{
  "name": "multi-source-data-quality",
  "executionMode": "batch",
  "description": "Extract from Kafka and MySQL, validate, transform, and load to DeltaLake and PostgreSQL",
  "steps": [
    {
      "stepType": "extract",
      "method": "fromKafka",
      "config": {
        "topic": "events",
        "streaming": false,
        "startingOffsets": "earliest",
        "credentialPath": "secret/data/kafka/dev",
        "credentialType": "other"
      }
    },
    {
      "stepType": "validate",
      "method": "validateSchema",
      "config": {
        "schema": {
          "type": "struct",
          "fields": [
            {"name": "event_id", "type": "string", "nullable": false},
            {"name": "timestamp", "type": "long", "nullable": false},
            {"name": "user_id", "type": "integer", "nullable": false}
          ]
        }
      }
    },
    {
      "stepType": "validate",
      "method": "validateNulls",
      "config": {
        "requiredColumns": ["event_id", "timestamp", "user_id"]
      }
    },
    {
      "stepType": "transform",
      "method": "enrichData",
      "config": {
        "columns": {
          "event_date": "to_date(from_unixtime(timestamp))",
          "event_hour": "hour(from_unixtime(timestamp))"
        }
      }
    },
    {
      "stepType": "transform",
      "method": "aggregateData",
      "config": {
        "groupBy": ["user_id", "event_date"],
        "aggregations": {
          "event_count": "count",
          "first_event": "min(timestamp)",
          "last_event": "max(timestamp)"
        }
      }
    },
    {
      "stepType": "load",
      "method": "toDeltaLake",
      "config": {
        "path": "/data/deltalake/user_events_daily",
        "mode": "Append",
        "mergeSchema": true
      }
    }
  ]
}
```

### Streaming Pipeline Example

**File**: `config/examples/streaming-kafka-delta.json`

```json
{
  "name": "streaming-kafka-to-delta",
  "executionMode": "streaming",
  "steps": [
    {
      "stepType": "extract",
      "method": "fromKafka",
      "config": {
        "topic": "real-time-events",
        "streaming": true,
        "startingOffsets": "latest",
        "credentialPath": "secret/data/kafka/dev",
        "credentialType": "other"
      }
    },
    {
      "stepType": "transform",
      "method": "filterRows",
      "config": {
        "condition": "event_type IN ('click', 'purchase', 'view')"
      }
    },
    {
      "stepType": "transform",
      "method": "enrichData",
      "config": {
        "columns": {
          "processing_time": "current_timestamp()",
          "partition_date": "date_format(current_timestamp(), 'yyyy-MM-dd')"
        }
      }
    },
    {
      "stepType": "load",
      "method": "toDeltaLake",
      "config": {
        "path": "/data/deltalake/events_stream",
        "mode": "Append",
        "checkpointLocation": "/data/checkpoints/events_stream"
      }
    }
  ]
}
```

## Configuration with External References

For complex pipelines, you can split configuration into multiple files:

**Main Pipeline**: `config/production-pipeline.json`
```json
{
  "name": "production-etl",
  "executionMode": "batch",
  "steps": [
    {
      "stepType": "extract",
      "method": "fromPostgres",
      "configRef": "sources/postgres-config.json"
    },
    {
      "stepType": "transform",
      "method": "aggregateData",
      "configRef": "transforms/aggregate-config.json"
    },
    {
      "stepType": "load",
      "method": "toS3",
      "configRef": "sinks/s3-config.json"
    }
  ]
}
```

**External Config**: `config/sources/postgres-config.json`
```json
{
  "table": "orders",
  "credentialPath": "secret/data/postgres/prod",
  "credentialType": "jdbc",
  "partitionColumn": "order_id",
  "numPartitions": 16,
  "lowerBound": 1,
  "upperBound": 1000000
}
```

## Available Extract Methods

### fromPostgres / fromMySQL
```json
{
  "table": "table_name",
  "credentialPath": "secret/data/postgres/env",
  "credentialType": "jdbc",
  "partitionColumn": "id",        // Optional: for parallel reads
  "numPartitions": 8,             // Optional: default 8
  "lowerBound": 1,                // Required if partitionColumn set
  "upperBound": 1000000           // Required if partitionColumn set
}
```

### fromKafka
```json
{
  "topic": "topic_name",
  "credentialPath": "secret/data/kafka/env",
  "credentialType": "other",
  "streaming": true,              // true for streaming, false for batch
  "startingOffsets": "earliest"   // "earliest" or "latest"
}
```

### fromS3
```json
{
  "bucket": "bucket_name",
  "path": "prefix/path/",
  "credentialPath": "secret/data/s3/env",
  "credentialType": "iam",
  "format": "parquet"             // parquet, csv, json, avro
}
```

### fromDeltaLake
```json
{
  "path": "/path/to/delta/table",
  "versionAsOf": 5,               // Optional: read specific version
  "timestampAsOf": "2024-01-15T10:00:00Z"  // Optional: time travel
}
```

## Available Transform Methods

### filterRows
```json
{
  "condition": "age > 18 AND status = 'active'"
}
```

### aggregateData
```json
{
  "groupBy": ["category", "region"],
  "aggregations": {
    "total_sales": "sum(amount)",
    "avg_price": "avg(price)",
    "order_count": "count(*)"
  }
}
```

### enrichData
```json
{
  "columns": {
    "full_name": "concat(first_name, ' ', last_name)",
    "year": "year(order_date)",
    "is_premium": "CASE WHEN amount > 1000 THEN true ELSE false END"
  }
}
```

### joinDataFrames
```json
{
  "joinKey": "user_id",
  "broadcast": true               // For small lookup tables
}
```
**Note**: `rightDataFrame` must be provided programmatically for now (future enhancement: support loading from config).

### reshapeData
```json
{
  "operation": "pivot",
  "pivotKey": "region",
  "pivotCol": "product"
}
```

## Available Validation Methods

### validateSchema
```json
{
  "schema": {
    "type": "struct",
    "fields": [
      {"name": "id", "type": "integer", "nullable": false},
      {"name": "name", "type": "string", "nullable": true}
    ]
  }
}
```

### validateNulls
```json
{
  "requiredColumns": ["id", "email", "created_at"]
}
```

### validateRanges
```json
{
  "ranges": {
    "age": [0, 120],
    "price": [0.01, 999999.99],
    "quantity": [1, 10000]
  }
}
```

### validateReferentialIntegrity
```json
{
  "foreignKey": "user_id"
}
```
**Note**: `referenceTable` must be provided programmatically.

### validateBusinessRules
```json
{
  "rules": [
    "total_amount = unit_price * quantity",
    "order_date <= ship_date",
    "discount_percent >= 0 AND discount_percent <= 100"
  ]
}
```

## Available Load Methods

### toPostgres / toMySQL
```json
{
  "table": "destination_table",
  "credentialPath": "secret/data/postgres/env",
  "credentialType": "jdbc",
  "mode": "Append"               // Append, Overwrite, ErrorIfExists, Ignore
}
```

### toKafka
```json
{
  "topic": "output_topic",
  "credentialPath": "secret/data/kafka/env",
  "credentialType": "other",
  "checkpointLocation": "/checkpoints/kafka_sink"  // Required for streaming
}
```

### toS3
```json
{
  "bucket": "output_bucket",
  "path": "output/path/",
  "credentialPath": "secret/data/s3/env",
  "credentialType": "iam",
  "format": "parquet",
  "mode": "Overwrite"
}
```

### toDeltaLake
```json
{
  "path": "/data/delta/table",
  "mode": "Append",
  "mergeSchema": true,            // Allow schema evolution
  "overwriteSchema": false
}
```

## Running Pipelines

### Local CLI Execution

For development and testing on your local machine:

```bash
# Build the application first
./gradlew build

# Run pipeline with default Spark configs (local mode)
./gradlew run --args="config/examples/simple-etl.json"

# Run with custom Spark master
./gradlew run --args="--master local[4] config/examples/simple-etl.json"

# Run with Spark configs
./gradlew run --args="--conf spark.executor.memory=4g config/examples/simple-etl.json"

# Run using standard JAR (local mode with embedded Spark)
java -jar build/libs/pipeline-app.jar config/examples/simple-etl.json
```

### Cluster Execution with spark-submit

For production deployment to Spark clusters:

```bash
# Build uber-JAR with dependencies (excludes Spark - provided by cluster)
./gradlew shadowJar

# Verify uber-JAR created
ls -lh build/libs/pipeline-app-all.jar

# Submit to standalone Spark cluster
spark-submit \
  --class com.pipeline.cli.PipelineRunner \
  --master spark://spark-master:7077 \
  --deploy-mode cluster \
  --driver-memory 2G \
  --executor-memory 4G \
  --executor-cores 4 \
  --total-executor-cores 16 \
  --conf spark.driver.extraJavaOptions="-DVAULT_ADDR=https://vault.prod.company.com" \
  --conf spark.executor.extraJavaOptions="-DVAULT_TOKEN=${VAULT_TOKEN}" \
  build/libs/pipeline-app-all.jar \
  /path/to/pipeline-config.json

# Submit to YARN cluster
spark-submit \
  --class com.pipeline.cli.PipelineRunner \
  --master yarn \
  --deploy-mode cluster \
  --driver-memory 2G \
  --executor-memory 8G \
  --num-executors 10 \
  --conf spark.yarn.submit.waitAppCompletion=true \
  --conf spark.driver.extraJavaOptions="-DVAULT_ADDR=$VAULT_ADDR -DVAULT_TOKEN=$VAULT_TOKEN" \
  --conf spark.executor.extraJavaOptions="-DVAULT_ADDR=$VAULT_ADDR" \
  --files /path/to/additional-config.json \
  build/libs/pipeline-app-all.jar \
  hdfs:///configs/production-pipeline.json

# Submit to Kubernetes (Spark on K8s)
spark-submit \
  --class com.pipeline.cli.PipelineRunner \
  --master k8s://https://kubernetes.default.svc:443 \
  --deploy-mode cluster \
  --name pipeline-job \
  --conf spark.executor.instances=5 \
  --conf spark.kubernetes.container.image=my-registry/spark:3.5.6 \
  --conf spark.kubernetes.authenticate.driver.serviceAccountName=spark \
  --conf spark.driver.extraJavaOptions="-DVAULT_ADDR=$VAULT_ADDR -DVAULT_TOKEN=$VAULT_TOKEN" \
  build/libs/pipeline-app-all.jar \
  s3a://configs/production-pipeline.json

# Submit to Databricks (via databricks CLI)
databricks jobs create --json '{
  "name": "Data Pipeline Job",
  "new_cluster": {
    "spark_version": "13.3.x-scala2.12",
    "node_type_id": "i3.xlarge",
    "num_workers": 8
  },
  "spark_jar_task": {
    "main_class_name": "com.pipeline.cli.PipelineRunner",
    "parameters": ["dbfs:/configs/pipeline.json"]
  },
  "libraries": [
    {"jar": "dbfs:/jars/pipeline-app-all.jar"}
  ]
}'

# Monitor job status
spark-submit \
  --status <driver-id> \
  --master spark://spark-master:7077

# Kill running job
spark-submit \
  --kill <driver-id> \
  --master spark://spark-master:7077
```

**Important spark-submit Notes**:
- Use `pipeline-app-all.jar` (uber-JAR) for cluster mode
- Spark dependencies are provided by the cluster
- Pass Vault credentials via `--conf` options
- Config file path can be local, HDFS, S3, or DBFS
- Use `--deploy-mode cluster` for production (driver runs on cluster)
- Use `--deploy-mode client` for debugging (driver runs locally)

### Programmatic Execution

```scala
import com.pipeline.core.Pipeline
import org.apache.spark.sql.SparkSession

val spark = SparkSession.builder()
  .appName("My Pipeline")
  .master("local[*]")
  .getOrCreate()

val pipeline = Pipeline.fromConfig("config/examples/simple-etl.json", spark)

pipeline.main(spark) match {
  case Success(_) => println("Pipeline succeeded!")
  case Failure(ex) => println(s"Pipeline failed: ${ex.getMessage}")
}

spark.stop()
```

## Multi-DataFrame Pipelines

For complex transformations requiring multiple DataFrames (joins, unions, lookups), use the `registerAs` and `inputDataFrames` configuration options.

### Example: Join Two Tables

```json
{
  "name": "customer-order-join",
  "executionMode": "batch",
  "steps": [
    {
      "stepType": "extract",
      "method": "fromPostgres",
      "config": {
        "table": "orders",
        "registerAs": "orders",
        "credentialPath": "secret/data/postgres/dev",
        "credentialType": "jdbc"
      }
    },
    {
      "stepType": "extract",
      "method": "fromPostgres",
      "config": {
        "table": "customers",
        "registerAs": "customers",
        "credentialPath": "secret/data/postgres/dev",
        "credentialType": "jdbc"
      }
    },
    {
      "stepType": "transform",
      "method": "joinDataFrames",
      "config": {
        "inputDataFrames": ["orders", "customers"],
        "joinKey": "customer_id",
        "joinType": "inner"
      }
    },
    {
      "stepType": "load",
      "method": "toS3",
      "config": {
        "bucket": "analytics",
        "path": "customer-orders/",
        "format": "parquet",
        "credentialPath": "secret/data/s3/dev",
        "credentialType": "iam"
      }
    }
  ]
}
```

### Example: Star Schema Join (Fact + Multiple Dimensions)

```json
{
  "name": "sales-star-schema",
  "executionMode": "batch",
  "steps": [
    {
      "stepType": "extract",
      "method": "fromPostgres",
      "config": {
        "table": "fact_sales",
        "registerAs": "sales",
        "credentialPath": "secret/data/postgres/prod",
        "credentialType": "jdbc"
      }
    },
    {
      "stepType": "extract",
      "method": "fromPostgres",
      "config": {
        "table": "dim_products",
        "registerAs": "products",
        "credentialPath": "secret/data/postgres/prod",
        "credentialType": "jdbc"
      }
    },
    {
      "stepType": "extract",
      "method": "fromPostgres",
      "config": {
        "table": "dim_stores",
        "registerAs": "stores",
        "credentialPath": "secret/data/postgres/prod",
        "credentialType": "jdbc"
      }
    },
    {
      "stepType": "extract",
      "method": "fromPostgres",
      "config": {
        "table": "dim_time",
        "registerAs": "time",
        "credentialPath": "secret/data/postgres/prod",
        "credentialType": "jdbc"
      }
    },
    {
      "stepType": "transform",
      "method": "joinDataFrames",
      "config": {
        "inputDataFrames": ["sales", "products"],
        "joinKey": "product_id",
        "joinType": "left",
        "registerAs": "sales_with_products"
      }
    },
    {
      "stepType": "transform",
      "method": "joinDataFrames",
      "config": {
        "inputDataFrames": ["sales_with_products", "stores"],
        "joinKey": "store_id",
        "joinType": "left",
        "registerAs": "sales_with_products_stores"
      }
    },
    {
      "stepType": "transform",
      "method": "joinDataFrames",
      "config": {
        "inputDataFrames": ["sales_with_products_stores", "time"],
        "joinKey": "date_id",
        "joinType": "left"
      }
    },
    {
      "stepType": "load",
      "method": "toDeltaLake",
      "config": {
        "path": "/data/warehouse/sales_fact_denormalized",
        "mode": "Overwrite"
      }
    }
  ]
}
```

### Example: Union Multiple Sources

```json
{
  "name": "multi-source-union",
  "executionMode": "batch",
  "steps": [
    {
      "stepType": "extract",
      "method": "fromPostgres",
      "config": {
        "table": "events_2023",
        "registerAs": "events_2023",
        "credentialPath": "secret/data/postgres/archive",
        "credentialType": "jdbc"
      }
    },
    {
      "stepType": "extract",
      "method": "fromPostgres",
      "config": {
        "table": "events_2024",
        "registerAs": "events_2024",
        "credentialPath": "secret/data/postgres/archive",
        "credentialType": "jdbc"
      }
    },
    {
      "stepType": "extract",
      "method": "fromPostgres",
      "config": {
        "table": "events_2025",
        "registerAs": "events_2025",
        "credentialPath": "secret/data/postgres/current",
        "credentialType": "jdbc"
      }
    },
    {
      "stepType": "transform",
      "method": "unionDataFrames",
      "config": {
        "inputDataFrames": ["events_2023", "events_2024", "events_2025"]
      }
    },
    {
      "stepType": "transform",
      "method": "aggregateData",
      "config": {
        "groupBy": ["event_type", "year"],
        "aggregations": {
          "total_events": "count(*)",
          "unique_users": "count(distinct user_id)"
        }
      }
    },
    {
      "stepType": "load",
      "method": "toS3",
      "config": {
        "bucket": "analytics",
        "path": "event-summary/",
        "format": "parquet",
        "mode": "Overwrite",
        "credentialPath": "secret/data/s3/prod",
        "credentialType": "iam"
      }
    }
  ]
}
```

### Multi-DataFrame Configuration Options

- **registerAs** (extract/transform steps): Register step output with a name for later reference
  ```json
  {
    "stepType": "extract",
    "method": "fromPostgres",
    "config": {
      "table": "users",
      "registerAs": "users"
    }
  }
  ```

- **inputDataFrames** (transform steps): List of registered DataFrame names to use as inputs
  ```json
  {
    "stepType": "transform",
    "method": "joinDataFrames",
    "config": {
      "inputDataFrames": ["orders", "users"]
    }
  }
  ```

- **Primary Data Flow**: The main DataFrame flows through the pipeline via Chain of Responsibility
- **DataFrame Registry**: Named DataFrames available for multi-input operations
- **Backward Compatible**: Single-DataFrame pipelines work without these options

### Multi-DataFrame Transform Methods

#### joinDataFrames
```json
{
  "method": "joinDataFrames",
  "config": {
    "inputDataFrames": ["left_df", "right_df"],
    "joinKey": "id",
    "joinType": "inner"  // inner, left, right, outer, left_anti, left_semi
  }
}
```

#### unionDataFrames
```json
{
  "method": "unionDataFrames",
  "config": {
    "inputDataFrames": ["df1", "df2", "df3"]
  }
}
```

## Testing

### Run All Tests

```bash
# Run all tests (unit + integration + contract + performance)
./gradlew test

# Run only unit tests
./gradlew test --tests "com.pipeline.unit.*"

# Run only integration tests
./gradlew test --tests "com.pipeline.integration.*"

# Run specific test class
./gradlew test --tests "com.pipeline.unit.core.PipelineStepTest"
```

### Run Integration Tests

Integration tests require Docker services to be running:

```bash
# Start services
docker-compose up -d

# Wait for services to be healthy
docker-compose ps | grep healthy

# Run integration tests
./gradlew test --tests "com.pipeline.integration.*"

# Stop services
docker-compose down
```

### Performance Testing

```bash
# Run performance benchmarks
./gradlew test --tests "com.pipeline.performance.*"

# Generate performance report
./gradlew performanceReport
```

Expected throughput (per spec requirements):
- **Simple batch**: 100K records/sec minimum
- **Complex batch**: 10K records/sec minimum
- **Streaming**: 50K events/sec, p95 latency < 5 seconds

## Monitoring & Debugging

### Structured Logs

All pipeline execution emits structured JSON logs:

```json
{
  "timestamp": "2025-10-13T10:15:30.123Z",
  "level": "INFO",
  "logger": "com.pipeline.core.Pipeline",
  "message": "Step completed: 1000 records in 523ms",
  "executionId": "a1b2c3d4-e5f6-7890-abcd-ef1234567890",
  "pipelineName": "simple-postgres-to-s3",
  "stepIndex": "0",
  "stepType": "extract",
  "recordCount": 1000,
  "durationMs": 523
}
```

### View Logs

```bash
# Follow logs in real-time
./gradlew run --args="config/examples/simple-etl.json" | jq .

# Filter for errors only
./gradlew run --args="config/examples/simple-etl.json" 2>&1 | jq 'select(.level == "ERROR")'

# Extract execution metrics
./gradlew run --args="config/examples/simple-etl.json" 2>&1 | jq '{step: .stepType, records: .recordCount, duration: .durationMs}'
```

### Spark UI

Access Spark UI for detailed execution metrics:

```bash
# Default URL when running locally
open http://localhost:4040
```

Spark UI provides:
- Job execution timeline
- Stage and task details
- Memory and CPU usage
- SQL query plans

## Troubleshooting

### Common Issues

#### 1. Vault Connection Failure
```
RuntimeException: Failed to read secret from Vault at path: secret/data/postgres/dev
```

**Solution**:
```bash
# Verify Vault is running
docker-compose ps vault

# Check Vault token
echo $VAULT_TOKEN

# Verify secret exists
docker exec -it vault-dev vault kv get secret/postgres/dev

# Re-initialize Vault
./docker/vault/init-vault.sh
```

#### 2. JDBC Connection Timeout
```
java.sql.SQLException: Connection refused (Connection refused)
```

**Solution**:
```bash
# Verify database is running
docker-compose ps postgres

# Check connection from host
psql -h localhost -p 5432 -U testuser -d testdb

# Verify credentials in Vault match docker-compose
```

#### 3. Kafka Consumer Lag
```
WARN ConsumerCoordinator: Could not find committed offset for partition events-0
```

**Solution**:
```json
// Add explicit starting offset to config
{
  "topic": "events",
  "startingOffsets": "earliest",  // or "latest"
  "failOnDataLoss": false
}
```

#### 4. Out of Memory Error
```
java.lang.OutOfMemoryError: Java heap space
```

**Solution**:
```bash
# Increase Spark memory
./gradlew run --args="--conf spark.executor.memory=8g --conf spark.driver.memory=4g config/pipeline.json"

# Add partitioning to extract step (see config examples above)
```

#### 5. Schema Mismatch
```
IllegalStateException: Schema mismatch: expected StructType(...), got StructType(...)
```

**Solution**:
```json
// Use mergeSchema option for DeltaLake
{
  "stepType": "load",
  "method": "toDeltaLake",
  "config": {
    "path": "/data/delta/table",
    "mergeSchema": true,        // Allow schema evolution
    "mode": "Append"
  }
}
```

## Performance Tuning

### Partitioning for Large Datasets

```json
{
  "stepType": "extract",
  "method": "fromPostgres",
  "config": {
    "table": "large_table",
    "partitionColumn": "id",
    "numPartitions": 16,           // More partitions = more parallelism
    "lowerBound": 1,
    "upperBound": 10000000
  }
}
```

### Broadcast Joins for Small Tables

```json
{
  "stepType": "transform",
  "method": "joinDataFrames",
  "config": {
    "joinKey": "category_id",
    "broadcast": true              // Broadcast small lookup table
  }
}
```

### Caching Intermediate Results

For pipelines that reuse intermediate results, add caching (future enhancement - currently manual in code):

```scala
// In custom pipeline code
val intermediate = step1.execute(data, spark)
intermediate match {
  case Right(df) => df.cache()  // Cache for reuse
  case _ => ()
}
```

### Monitoring Resource Usage

```bash
# Monitor Docker container resources
docker stats

# Monitor Spark executor metrics (via Spark UI)
open http://localhost:4040/executors/
```

## Production Deployment

### Environment-Specific Configurations

Create separate `.env` files for each environment:

```bash
.env.dev       # Development environment
.env.staging   # Staging environment
.env.prod      # Production environment
```

Load environment-specific file:

```bash
source .env.prod
./gradlew run --args="config/production-pipeline.json"
```

### Vault Token Rotation

For production, use AppRole authentication instead of static tokens:

```bash
# Enable AppRole auth
vault auth enable approle

# Create AppRole for pipeline
vault write auth/approle/role/pipeline-role \
  token_ttl=1h \
  token_max_ttl=4h \
  policies="pipeline-policy"

# Get RoleID and SecretID
ROLE_ID=$(vault read -field=role_id auth/approle/role/pipeline-role/role-id)
SECRET_ID=$(vault write -field=secret_id -f auth/approle/role/pipeline-role/secret-id)

# Authenticate with AppRole (code update required)
```

### Monitoring in Production

Integrate with log aggregation and metrics systems:

- **Logs**: Ship structured JSON logs to ELK, Splunk, or CloudWatch
- **Metrics**: Export Spark metrics to Prometheus, Datadog, or Grafana
- **Alerting**: Set up alerts for pipeline failures, slow execution, validation errors

### CI/CD Integration

```yaml
# Example GitHub Actions workflow
name: Pipeline Tests

on: [push, pull_request]

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - uses: actions/setup-java@v3
        with:
          java-version: '11'
      - name: Start Docker services
        run: docker-compose up -d
      - name: Run tests
        run: ./gradlew test
      - name: Stop Docker services
        run: docker-compose down
```

## Next Steps

1. **Review Spec**: Read [spec.md](spec.md) for complete requirements
2. **Review Data Model**: Read [data-model.md](data-model.md) for entity details
3. **Review Contracts**: Check [contracts/](contracts/) for JSON schemas
4. **Run Examples**: Try all examples in `config/examples/`
5. **Create Custom Pipeline**: Build your own pipeline config
6. **Run Performance Tests**: Validate throughput meets requirements
7. **Deploy to Production**: Follow production deployment guide above

## Support

For issues or questions:
- Check troubleshooting section above
- Review logs with `jq` for structured analysis
- Consult Spark UI for execution details
- Open GitHub issue with pipeline config and logs

---

**Documentation Version**: 1.0.0 | **Last Updated**: 2025-10-13
