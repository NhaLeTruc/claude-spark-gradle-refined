# Pipeline Application Configuration

# Spark Configuration (defaults for local CLI mode)
spark {
  app.name = "Pipeline Orchestration App"
  master = "local[*]"

  # Memory settings
  driver.memory = "2g"
  executor.memory = "2g"

  # Serialization
  serializer = "org.apache.spark.serializer.KryoSerializer"
  kryo.registrationRequired = false

  # SQL settings
  sql.adaptive.enabled = true
  sql.adaptive.coalescePartitions.enabled = true

  # Delta Lake
  sql.extensions = "io.delta.sql.DeltaSparkSessionExtension"
  sql.catalog.spark_catalog = "org.apache.spark.sql.delta.catalog.DeltaCatalog"

  # S3 configuration
  hadoop.fs.s3a.impl = "org.apache.hadoop.fs.s3a.S3AFileSystem"
  hadoop.fs.s3a.path.style.access = true
  hadoop.fs.s3a.connection.ssl.enabled = false
}

# Vault Configuration
vault {
  address = "http://localhost:8200"
  address = ${?VAULT_ADDR}

  token = ""
  token = ${?VAULT_TOKEN}

  namespace = ""
  namespace = ${?VAULT_NAMESPACE}

  # Credential paths
  paths {
    postgres = "secret/data/postgres"
    mysql = "secret/data/mysql"
    kafka = "secret/data/kafka"
    s3 = "secret/data/s3"
    deltalake = "secret/data/deltalake"
  }
}

# Pipeline Configuration
pipeline {
  # Retry settings (FR-016)
  retry {
    maxAttempts = 3
    delayMillis = 5000
  }

  # Batch processing settings
  batch {
    # Default batch size for partitioning
    batchSize = 10000

    # Read/write parallelism
    parallelism = 4
  }

  # Streaming settings
  streaming {
    # Micro-batch interval
    batchIntervalSeconds = 5

    # Checkpoint location
    checkpointDir = "/tmp/pipeline-checkpoints"
    checkpointDir = ${?CHECKPOINT_DIR}

    # Max offsets per trigger for Kafka
    maxOffsetsPerTrigger = 50000
  }
}

# Observability
observability {
  # Enable structured logging with MDC
  structuredLogging = true

  # Correlation ID for tracing
  correlationIdHeader = "X-Correlation-ID"

  # Metrics collection
  metricsEnabled = true
}
